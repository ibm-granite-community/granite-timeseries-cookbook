{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8d62ab",
   "metadata": {},
   "source": [
    "# Retail Sales Forecasting using the M5 dataset with Granite Time Series - Few-shot finetuning, evaluation, and visualization\n",
    "\n",
    "In this tutorial, we will explore [timeseries forecasting](https://www.ibm.com/think/insights/time-series-forecasting) using the [IBM Granite Timeseries model](https://ibm.com/granite) to predict retail sales. We will cover key techniques such as few-shot forecasting and fine-tuning. We are using  [M5 datasets](https://drive.google.com/drive/folders/1D6EWdVSaOtrP1LEFh1REjI3vej6iUS_4?usp=sharing) from the official M-Competitions [repository](https://github.com/Mcompetitions/M5-methods) to forecast future sales aggregated by state. The aim of this recipe is to showcase how to use a pre-trained time series foundation model for multivariate forecasting and explores various features available with Granite Time Series Foundation Models.\n",
    "\n",
    "This recipe uses TinyTimeMixers (TTMs), which are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. With less than 1 Million parameters, TTM introduces the notion of the first-ever \"tiny\" pre-trained models for Time-Series Forecasting. TTM outperforms several popular benchmarks demanding billions of parameters in zero-shot and few-shot forecasting and can easily be fine-tuned for multivariate forecasts.\n",
    "\n",
    "## Setting Up\n",
    "\n",
    "### Install the TSFM Library\n",
    "\n",
    "The [granite-tsfm library](https://github.com/ibm-granite/granite-tsfm) provides utilities for working with Time Series Foundation Models (TSFM). Here we retrieve and install the latest version of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772bb9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install the tsfm library\n",
    "! pip install \"granite-tsfm[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.22\"\n",
    "# Install a utility to help download data files from google drive during the data prep process\n",
    "! pip install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b62e9",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "From `tsfm_public`, we use the TinyTimeMixer model, forecasting pipeline, and plotting function. We also leverage a few components for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5230e2-b163-4d1a-a715-239b9190cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Subset\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "\n",
    "from tsfm_public import (\n",
    "    ForecastDFDataset,\n",
    "    TimeSeriesForecastingPipeline,\n",
    "    TimeSeriesPreprocessor,\n",
    "    TinyTimeMixerForPrediction,\n",
    "    TrackingCallback,\n",
    "    count_parameters,\n",
    ")\n",
    "from tsfm_public.toolkit.lr_finder import optimal_lr_finder\n",
    "from tsfm_public.toolkit.time_series_preprocessor import prepare_data_splits\n",
    "from tsfm_public.toolkit.util import select_by_timestamp\n",
    "from tsfm_public.toolkit.visualization import plot_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcdc5f",
   "metadata": {},
   "source": [
    "### Specify configuration variables\n",
    "\n",
    "The forecast length is specified as well as the context length (in time steps) which is set to match the pretrained model. Additionally, we declare the Granite Time Series Model and the specific revision we are targeting.\n",
    "\n",
    "The granite-timeseries TTM R2 card has several different revisions of the model available for various context lengths and prediction lengths. In this example we will be working with daily data, so we choose a model suitable for that resolution -- 90 days of history to forecast the next 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d6461",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_length = 28\n",
    "context_length = 90\n",
    "\n",
    "TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
    "REVISION = \"90-30-ft-l1-r2.1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd3add-8948-453e-bd1c-13977979ecee",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "As mentioned in the introduction, this notebook makes use of the [M5 datasets](https://drive.google.com/drive/folders/1D6EWdVSaOtrP1LEFh1REjI3vej6iUS_4?usp=sharing) from the official M-Competitions [repository](https://github.com/Mcompetitions/M5-methods). \n",
    "\n",
    "\n",
    "The original data includes hierarchy and product information. For this example we aggregate the sales by state into three separate time series. The code for downloading the datasets and preparing them is available in `M5_retail_data_prep.py`. Here, we first make sure we have access to the `M5_retail_data_prep.py` file (in an environment like colab, we need to download the file) and then we simply run the `prepare_data()` function to save the prepared dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def download_file(file_url, destination):\n",
    "    if os.path.exists(destination):\n",
    "        return\n",
    "    response = requests.get(file_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(destination, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        # logger.info(f\"Downloaded: {destination}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {file_url}. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "download_file(\n",
    "    file_url=\"https://raw.githubusercontent.com/ibm-granite-community/granite-timeseries-cookbook/refs/heads/main/recipes/Retail_Forecasting/M5_retail_data_prep.py\",\n",
    "    destination=\"./M5_retail_data_prep.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37127509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the file we just made sure we had access to, import the data prep method\n",
    "from M5_retail_data_prep import prepare_data\n",
    "\n",
    "prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce78a17",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "\n",
    "We parse the CSV into a pandas dataframe and ensure the timestamp column is a UTC datetime and drop two unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99c39c-04d4-4df6-8bfe-500d4c973b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"m5_for_state_level_forecasting.csv.gz\"\n",
    "\n",
    "data = pd.read_csv(data_path, parse_dates=[\"date\"]).drop(columns=[\"d\", \"weekday\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c127c",
   "metadata": {},
   "source": [
    "Next, we must clean up the columns in our data and declare the names of the timestamp column, the target column to be predicted as well as the categorical column used to aggregate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d0d68-bd82-47d1-afab-fd4e7a3ffe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(data.columns)\n",
    "[cols.remove(c) for c in [\"date\", \"sales\", \"state_id\", \"state_id_cat\"]]\n",
    "cols\n",
    "\n",
    "column_specifiers = {\n",
    "    \"timestamp_column\": \"date\",\n",
    "    \"id_columns\": [\"state_id\"],\n",
    "    \"target_columns\": [\"sales\"],\n",
    "    \"control_columns\": cols,\n",
    "    \"static_categorical_columns\": [\"state_id_cat\"],\n",
    "    \"categorical_columns\": [\n",
    "        \"event_name_1\",\n",
    "        \"event_type_1\",\n",
    "        \"event_name_2\",\n",
    "        \"event_type_2\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4011a6cc",
   "metadata": {},
   "source": [
    "### Train the Preprocessor\n",
    "\n",
    "The preprocessor is trained on the training portion of the input data to learn the scaling factors. The scaling will be applied when we use the preprocess method of the time series preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa71ff5-f6ba-4c51-8f4f-9a12438845fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp = TimeSeriesPreprocessor(\n",
    "    **column_specifiers,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_length,\n",
    "    scaling=True,\n",
    "    encode_categorical=True,\n",
    "    scaler_type=\"standard\",\n",
    ")\n",
    "\n",
    "df_train = select_by_timestamp(\n",
    "    data, timestamp_column=column_specifiers[\"timestamp_column\"], end_timestamp=\"2016-05-23\"\n",
    ")\n",
    "\n",
    "trained_tsp = tsp.train(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543737f6",
   "metadata": {},
   "source": [
    "## Finetune the model\n",
    "\n",
    "Now we will focus on fine-tuning the pretrained model. We use the same data splits we defined above, but now include extra columns during the fine-tuning process.\n",
    "\n",
    "### Preparing the data for fine-tuning\n",
    "\n",
    "We split the data into training, validation, and test sets. The training set is used to train the model, while the test set is used to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_params = {\"train\": 0.5, \"test\": 0.25}\n",
    "\n",
    "train_data, valid_data, test_data = prepare_data_splits(\n",
    "    data, id_columns=column_specifiers[\"id_columns\"], split_config=split_params, context_length=context_length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6279d1",
   "metadata": {},
   "source": [
    "Here we will construct the torch dataset because we cant pass panda dataframes using our torch dataset class specifically designed for forecasting usecases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665fa12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_token = tsp.get_frequency_token(tsp.freq)\n",
    "\n",
    "dataset_params = column_specifiers.copy()\n",
    "dataset_params[\"frequency_token\"] = frequency_token\n",
    "dataset_params[\"context_length\"] = context_length\n",
    "dataset_params[\"prediction_length\"] = forecast_length\n",
    "\n",
    "\n",
    "train_dataset = ForecastDFDataset(tsp.preprocess(train_data), **dataset_params)\n",
    "valid_dataset = ForecastDFDataset(tsp.preprocess(valid_data), **dataset_params)\n",
    "test_dataset = ForecastDFDataset(tsp.preprocess(test_data), **dataset_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f9cce",
   "metadata": {},
   "source": [
    "Now let's take a smaller sample from the torch datasets produced above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% training and validation data (few-shot finetuning)\n",
    "fewshot_fraction = 0.20\n",
    "n_train_all = len(train_dataset)\n",
    "train_index = np.random.permutation(n_train_all)[: int(fewshot_fraction * n_train_all)]\n",
    "train_dataset = Subset(train_dataset, train_index)\n",
    "\n",
    "n_valid_all = len(valid_dataset)\n",
    "valid_index = np.random.permutation(n_valid_all)[: int(fewshot_fraction * n_valid_all)]\n",
    "valid_dataset = Subset(valid_dataset, valid_index)\n",
    "\n",
    "n_train_all, len(train_dataset), n_valid_all, len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22337b16-1e82-494a-bce8-8c49b9758cb2",
   "metadata": {},
   "source": [
    "### Load the model for fine-tuning\n",
    "\n",
    "We must first load the TTM model available on HuggingFace using the model and revision set above. We have one target channel, several exogenous channels, and one static categorical input. To take these into account, we use the `TimeSeriesPreprocessor` to provide the `prediction_channel_indices`, `exogenous_channel_indices`, and `categorical_vocab_size_list` information to the model. Note that we also enable channel mixing in the decoder and forecast channel mising. This allows the decoder to be tuned to capture interactions between the channels as well as adjust the forecasts based on interactions with the exogenous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c77656-42d4-4b51-a6c3-a4245a794cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1234)\n",
    "\n",
    "finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "    TTM_MODEL_PATH,\n",
    "    revision=REVISION,\n",
    "    context_length=context_length,\n",
    "    prediction_filter_length=forecast_length,\n",
    "    num_input_channels=tsp.num_input_channels,\n",
    "    decoder_mode=\"mix_channel\",  # exog:  set to mix_channel for mixing channels in history\n",
    "    prediction_channel_indices=tsp.prediction_channel_indices,\n",
    "    exogenous_channel_indices=tsp.exogenous_channel_indices,\n",
    "    fcm_context_length=1,  # exog: indicates lag length to use in the exog fusion. for Ex. if today sales can get affected by discount on +/- 2 days, mention 2\n",
    "    fcm_use_mixer=True,  # exog: Try true (1st option) or false\n",
    "    fcm_mix_layers=2,  # exog: Number of layers for exog mixing\n",
    "    enable_forecast_channel_mixing=True,  # exog: set true for exog mixing\n",
    "    categorical_vocab_size_list=tsp.categorical_vocab_size_list,  # sizes of the static categorical variables\n",
    "    fcm_prepend_past=True,  # exog: set true to include lag from history during exog infusion.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf182c",
   "metadata": {},
   "source": [
    "### Optional: Freeze the TTM Backbone\n",
    "\n",
    "Oftentimes, during fine-tuning we freeze the backbone and focus on tuning only the parameters in the decoder. This reduces the overall number of parameters being tuned and maintains what the encoder learned during pretraining.\n",
    "\n",
    "For this dataset, however, we found that performance was better when the backbone remained unfrozen -- for other datasets one might prefer to freeze the backbone. We have disabled the backbone freezing code, but left it intact as an example of what might need to be done for other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_backbone = False\n",
    "if freeze_backbone:\n",
    "    print(\n",
    "        \"Number of params before freezing backbone\",\n",
    "        count_parameters(finetune_forecast_model),\n",
    "    )\n",
    "\n",
    "    # Freeze the backbone of the model\n",
    "    for param in finetune_forecast_model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Count params\n",
    "    print(\n",
    "        \"Number of params after freezing the backbone\",\n",
    "        count_parameters(finetune_forecast_model),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d1996",
   "metadata": {},
   "source": [
    "### Set up a Trainer for Fine-tuning\n",
    "\n",
    "Configure a Trainer for use in fine-tuning and evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e625c63-0dff-4c13-85af-22e7848b5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate, finetune_forecast_model = optimal_lr_finder(\n",
    "    finetune_forecast_model,\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    enable_prefix_tuning=True,\n",
    ")\n",
    "print(\"OPTIMAL SUGGESTED LEARNING RATE =\", learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c4049d",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "Here we train the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39944f77-4bb7-4066-b5a7-6f3a76cb1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"ttm_finetuned_models/\"\n",
    "\n",
    "print(f\"Using learning rate = {learning_rate}\")\n",
    "finetune_forecast_args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUT_DIR, \"output\"),\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_epochs,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=2 * batch_size,\n",
    "    dataloader_num_workers=1,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_dir=os.path.join(OUT_DIR, \"logs\"),  # Make sure to specify a logging directory\n",
    "    load_best_model_at_end=True,  # Load the best model when training ends\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "    greater_is_better=False,  # For loss\n",
    "    use_cpu=device == \"cpu\",\n",
    ")\n",
    "\n",
    "# Create the early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "    early_stopping_threshold=0.0,  # Minimum improvement required to consider as improvement\n",
    ")\n",
    "tracking_callback = TrackingCallback()\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=math.ceil(len(train_dataset) / (batch_size)),\n",
    ")\n",
    "\n",
    "finetune_forecast_trainer = Trainer(\n",
    "    model=finetune_forecast_model,\n",
    "    args=finetune_forecast_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[early_stopping_callback, tracking_callback],\n",
    "    optimizers=(optimizer, scheduler),\n",
    ")\n",
    "\n",
    "# Fine tune\n",
    "finetune_forecast_trainer.train()\n",
    "\n",
    "finetune_forecast_trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2460da14",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "Evaluate the fine-tuned model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc24936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some standard metrics.\n",
    "def custom_metric(actual, prediction, column_header=\"results\"):\n",
    "    \"\"\"Simple function to compute MSE\"\"\"\n",
    "    a = np.asarray(actual.tolist())\n",
    "    p = np.asarray(prediction.tolist())\n",
    "    if p.shape[1] < a.shape[1]:\n",
    "        a = a[:, : p.shape[1]]\n",
    "\n",
    "    mask = ~np.any(np.isnan(a), axis=1)\n",
    "\n",
    "    mse = np.mean(np.square(a[mask, :] - p[mask, :]))\n",
    "    mae = np.mean(np.abs(a[mask, :] - p[mask, :]))\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            column_header: {\n",
    "                \"mean_squared_error\": mse,\n",
    "                \"root_mean_squared_error\": np.sqrt(mse),\n",
    "                \"mean_absolute_error\": mae,\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94067d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate forecasts using the finetuned model\n",
    "pipeline = TimeSeriesForecastingPipeline(\n",
    "    finetune_forecast_model,\n",
    "    device=device,  # Specify your local GPU or CPU.\n",
    "    feature_extractor=tsp,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# Make a forecast on the target column given the input data.\n",
    "finetune_forecast = pipeline(test_data)\n",
    "finetune_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metric(finetune_forecast[\"sales\"], finetune_forecast[\"sales_prediction\"], \"fine-tune forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c74b17",
   "metadata": {},
   "source": [
    "### Plot the Predictions vs. Actuals\n",
    "\n",
    "Plot the predictions vs. actuals for some random samples of time intervals in test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(\n",
    "    input_df=test_data[test_data.state_id == \"CA\"],\n",
    "    predictions_df=finetune_forecast[finetune_forecast.state_id == \"CA\"],\n",
    "    freq=\"d\",\n",
    "    timestamp_column=column_specifiers[\"timestamp_column\"],\n",
    "    channel=column_specifiers[\"target_columns\"][0],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
