{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transportation Demand Forecasting with Granite Time Series - Zero-shot Inference and Fine-tuning with Exogenous Inputs\n",
    "\n",
    "TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. With less than 1 Million parameters, TTM introduces the notion of the first-ever \"tiny\" pre-trained models for Time-Series Forecasting. TTM outperforms several popular benchmarks demanding billions of parameters in zero-shot and few-shot forecasting and can easily be fine-tuned for multivariate forecasts.\n",
    "\n",
    "In this recipe, we cover zero-shot forecasting, as well as fine-tuning. This example makes use of the Kaggle bike sharing [dataset](https://www.kaggle.com/datasets/lakshmi25npathi/bike-sharing-dataset) which contains bikes rental demand with weather and seasonal information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the TSFM Library \n",
    "\n",
    "The [granite-tsfm library](https://github.com/ibm-granite/granite-tsfm) provides utilities for working with Time Series Foundation Models (TSFM). Here we retrieve and install a tested version of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the tsfm library\n",
    "! pip install \"tsfm_public[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.17\" -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "From `tsfm_public`, we use the TinyTimeMixer model, forecasting pipeline, and plotting function. We also leverage a few components for the fine-tuning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "\n",
    "from tsfm_public import (\n",
    "    TimeSeriesForecastingPipeline,\n",
    "    TimeSeriesPreprocessor,\n",
    "    TinyTimeMixerForPrediction,\n",
    "    TrackingCallback,\n",
    "    count_parameters,\n",
    "    get_datasets,\n",
    ")\n",
    "from tsfm_public.toolkit.time_series_preprocessor import prepare_data_splits\n",
    "from tsfm_public.toolkit.visualization import plot_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify configuration variables\n",
    "\n",
    "We provide the names of the timestamp column and the target column to be predicted. The context length (in time steps) is set to match the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTM model branch\n",
    "# Use main for 512-96 model\n",
    "# Use \"1024-96-r2\" for 1024-96 model\n",
    "TTM_MODEL_REVISION = \"main\"\n",
    "context_length = 512  # the max context length for the 512-96 model\n",
    "prediction_length = 96  # the max prediction length for the 512-96 model\n",
    "\n",
    "# Output directory for writing evaluation results.\n",
    "OUT_DIR = \"/tmp/ttm_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "We'll work with a [bike sharing datasety](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset) available from the UCI Machine learning repository. This dataset includes the count of rental bikes between the years 2011 and 2012 in the Capital bike share system with the corresponding weather and seasonal information.\n",
    "\n",
    "You can download the source code to a temporary directory by running the following commands. Later you can clean up any downloaded files by removing the `temp` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# curl https://archive.ics.uci.edu/static/public/275/$BIKE_SHARING -o $BIKE_SHARING && \\\n",
    "BIKE_SHARING=bike+sharing+dataset.zip\n",
    "test -d temp || ( \\\n",
    "  mkdir -p temp && \\\n",
    "  cd temp && \\\n",
    "    wget https://archive.ics.uci.edu/static/public/275/$BIKE_SHARING -O $BIKE_SHARING && \\\n",
    "    unzip -o $BIKE_SHARING && \\\n",
    "  rm -f $BIKE_SHARING && \\\n",
    "  cd - \\\n",
    ") && ls -l temp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = \"temp/hour.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "\n",
    "We parse the CSV into a pandas dataframe, filling in any null values, and create a single window containing `context_length` time points. We ensure the timestamp column is a UTC datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_column = \"dteday\"\n",
    "target_columns = [\"casual\", \"registered\", \"cnt\"]\n",
    "\n",
    "# Read in the data from the downloaded file.\n",
    "input_df = pd.read_csv(DATA_FILE_PATH, parse_dates=[timestamp_column])\n",
    "\n",
    "# Fix missing hours in original dataset date column\n",
    "input_df[timestamp_column] = input_df[timestamp_column] + input_df.hr.apply(lambda x: pd.Timedelta(x, unit=\"hr\"))\n",
    "\n",
    "# Show the last few rows of the dataset.\n",
    "input_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a segment of the target series\n",
    "\n",
    "Here we inspect a preview of the target time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_column in target_columns:\n",
    "    input_df.iloc[1000 : 1000 + 24 * 10].plot(x=timestamp_column, y=target_column, figsize=(20, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot forecasting\n",
    "\n",
    "In the first part of this notebook, we focus on zero-shot inference of the targets only. This does not consider the relationships between the targets or any of the additional exogenous features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training, validation, and testing sets\n",
    "\n",
    "We split the data into training, validation, and test sets. The training set is used to train the preprocessor, while the test set is used to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_config = {\"train\": 0.5, \"test\": 0.25}\n",
    "\n",
    "train_df, valid_df, test_df = prepare_data_splits(input_df, context_length=context_length, split_config=split_config)\n",
    "print(f\"Data lengths: train = {len(train_df)}, val = {len(valid_df)}, test = {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Preprocessor\n",
    "\n",
    "The preprocessor is trained on the training portion of the input data to learn the scaling factors. The scaling will be applied when we use the preprocess method of the time series preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp = TimeSeriesPreprocessor(\n",
    "    timestamp_column=timestamp_column,\n",
    "    target_columns=target_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=prediction_length,\n",
    "    scaling=True,\n",
    "    scaler_type=\"standard\",\n",
    ")\n",
    "\n",
    "trained_tsp = tsp.train(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate zero-shot forecasts and evaluate\n",
    "\n",
    "To generate forecasts from the zero-shot model we first load the model and then configure a TimeSeriesForecastingPipeline. The pipeline is responsible for using the zero-shot model and the preprocessor to create forecasts and output the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "    \"ibm-granite/granite-timeseries-ttm-r2\",  # Name of the model on HuggingFace.\n",
    "    revision=TTM_MODEL_REVISION,\n",
    "    num_input_channels=len(target_columns),  # Number of input columns.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = TimeSeriesForecastingPipeline(\n",
    "    zeroshot_model,\n",
    "    device=\"cpu\",  # Specify your local GPU or CPU.\n",
    "    feature_extractor=tsp,\n",
    ")\n",
    "\n",
    "# Make a forecast on the target column given the input data.\n",
    "zeroshot_forecast = pipeline(test_df)\n",
    "zeroshot_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some standard metrics.\n",
    "def custom_metric(actual, prediction, column_header=\"results\"):\n",
    "    \"\"\"Simple function to compute MSE\"\"\"\n",
    "    a = np.asarray(actual.tolist())\n",
    "    p = np.asarray(prediction.tolist())\n",
    "\n",
    "    mask = ~np.any(np.isnan(a), axis=1)\n",
    "\n",
    "    mse = np.mean(np.square(a[mask, :] - p[mask, :]))\n",
    "    mae = np.mean(np.abs(a[mask, :] - p[mask, :]))\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            column_header: {\n",
    "                \"mean_squared_error\": mse,\n",
    "                \"root_mean_squared_error\": np.sqrt(mse),\n",
    "                \"mean_absolute_error\": mae,\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "custom_metric(zeroshot_forecast[\"cnt\"], zeroshot_forecast[\"cnt_prediction\"], \"zero-shot forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Predictions vs. Actuals\n",
    "\n",
    "Plot the predictions vs. actuals for some random samples of time intervals in test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(\n",
    "    input_df=test_df,\n",
    "    predictions_df=zeroshot_forecast,\n",
    "    freq=\"h\",\n",
    "    timestamp_column=timestamp_column,\n",
    "    channel=target_columns[2],\n",
    "    indices=[1974, 570, 722],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "In this part of this notebook, we focus on fine-tuning the pretrained model. We use the same data splits we defined above, but now include extra columns during the fine-tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for fine-tuning\n",
    "\n",
    "We split the data into training, validation, and test sets. The training set is used to train the model, while the test set is used to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "control_columns = [\n",
    "    \"season\",\n",
    "    \"yr\",\n",
    "    \"mnth\",\n",
    "    \"holiday\",\n",
    "    \"weekday\",\n",
    "    \"workingday\",\n",
    "    \"weathersit\",\n",
    "    \"temp\",\n",
    "    \"atemp\",\n",
    "    \"hum\",\n",
    "    \"windspeed\",\n",
    "]\n",
    "\n",
    "tsp = TimeSeriesPreprocessor(\n",
    "    timestamp_column=timestamp_column,\n",
    "    target_columns=target_columns,\n",
    "    control_columns=control_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=prediction_length,\n",
    "    scaling=True,\n",
    "    encode_categorical=False,\n",
    "    scaler_type=\"standard\",\n",
    ")\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = get_datasets(\n",
    "    tsp,\n",
    "    input_df,\n",
    "    split_config,\n",
    ")\n",
    "print(f\"Data lengths: train = {len(train_dataset)}, val = {len(valid_dataset)}, test = {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model for fine-tuning\n",
    "Similar to the zero-shot case, we load the TTM model available on [HuggingFace](https://huggingface.co/ibm-granite/granite-timeseries-ttm-v1). We have three target channels and several exogenous channels in this example and set configuration appropriately take this into accounts. Note that we also enable channel mixing in the decoder and forecast channel mising. This allows the decoder to be tuned to capture interactions between the channels as well as adjust the forecasts based on interactions with the exogenous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "    \"ibm-granite/granite-timeseries-ttm-r2\",\n",
    "    revision=TTM_MODEL_REVISION,\n",
    "    num_input_channels=tsp.num_input_channels,\n",
    "    decoder_mode=\"mix_channel\",  # exog:  set to mix_channel for mixing channels in history\n",
    "    prediction_channel_indices=tsp.prediction_channel_indices,\n",
    "    exogenous_channel_indices=tsp.exogenous_channel_indices,\n",
    "    fcm_context_length=1,  # exog: indicates lag length to use in the exog fusion. for Ex. if today sales can get affected by discount on +/- 2 days, mention 2\n",
    "    fcm_use_mixer=True,  # exog: Try true (1st option) or false\n",
    "    fcm_mix_layers=2,  # exog: Number of layers for exog mixing\n",
    "    enable_forecast_channel_mixing=True,  # exog: set true for exog mixing\n",
    "    fcm_prepend_past=True,  # exog: set true to include lag from history during exog infusion.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the TTM Backbone\n",
    "\n",
    "During fine-tuning we freeze the backbone and focus on tuning only the parameters in the decoder. This reduces the overall number of parameters being tuned and maintains what the encoder learned during pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Number of params before freezing backbone\",\n",
    "    count_parameters(finetune_forecast_model),\n",
    ")\n",
    "\n",
    "# Freeze the backbone of the model\n",
    "for param in finetune_forecast_model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Count params\n",
    "print(\n",
    "    \"Number of params after freezing the backbone\",\n",
    "    count_parameters(finetune_forecast_model),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Trainer for Fine-tuning\n",
    "\n",
    "Configure a Trainer for use in fine-tuning and evaluating the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important parameters\n",
    "learning_rate = 0.0003  # 0.000298364724028334\n",
    "num_epochs = 50  # Ideally, we need more epochs (try offline preferably in a gpu for faster computation)\n",
    "batch_size = 64\n",
    "\n",
    "print(f\"Using learning rate = {learning_rate}\")\n",
    "finetune_forecast_args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUT_DIR, \"output\"),\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_epochs,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=None,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_dir=os.path.join(OUT_DIR, \"logs\"),  # Make sure to specify a logging directory\n",
    "    load_best_model_at_end=True,  # Load the best model when training ends\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "    greater_is_better=False,  # For loss\n",
    "    use_cpu=True,  # Remove when GPU is available\n",
    ")\n",
    "\n",
    "# Create the early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "    early_stopping_threshold=0.0,  # Minimum improvement required to consider as improvement\n",
    ")\n",
    "tracking_callback = TrackingCallback()\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=math.ceil(len(train_dataset) / (batch_size)),\n",
    ")\n",
    "\n",
    "finetune_forecast_trainer = Trainer(\n",
    "    model=finetune_forecast_model,\n",
    "    args=finetune_forecast_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[early_stopping_callback, tracking_callback],\n",
    "    optimizers=(optimizer, scheduler),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "Here we train the model on the training data. This tunes only the weights in the decoder and output layers, as the other weights have been frozen, taking into account the interactions between all the channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune\n",
    "finetune_forecast_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "Evaluate the fine-tuned model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate forecasts using the finetuned model\n",
    "pipeline = TimeSeriesForecastingPipeline(\n",
    "    finetune_forecast_model,\n",
    "    device=\"cpu\",  # Specify your local GPU or CPU.\n",
    "    feature_extractor=tsp,\n",
    ")\n",
    "\n",
    "# Make a forecast on the target column given the input data.\n",
    "finetune_forecast = pipeline(test_df)\n",
    "finetune_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metric(finetune_forecast[\"cnt\"], finetune_forecast[\"cnt_prediction\"], \"fine-tune forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Predictions vs. Actuals\n",
    "\n",
    "Plot the predictions vs. actuals for some random samples of time intervals in test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plot_predictions(\n",
    "    input_df=test_df,\n",
    "    predictions_df=finetune_forecast,\n",
    "    freq=\"h\",\n",
    "    timestamp_column=timestamp_column,\n",
    "    channel=target_columns[2],\n",
    "    indices=[1974, 570, 722],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a forecasting pipeline with the fine-tuned model\n",
    "\n",
    "We extract the appropriate historical data (context) and future values of control variables, and then set up the forecasting pipeline with the model and time series preprocessor.\n",
    "\n",
    "We choose a context window near the end of the test dataset, while still leaving enough known data from which we extract the controls. This is meant to simulate the case where you have historical information for the inputs and also provide future values of the controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df = test_df.iloc[-prediction_length-context_length:-prediction_length].copy()\n",
    "controls_df = test_df.iloc[-prediction_length:][[timestamp_column] + control_columns].copy()\n",
    "\n",
    "# generate forecasts using the finetuned model\n",
    "pipeline = TimeSeriesForecastingPipeline(\n",
    "    finetune_forecast_model,\n",
    "    device=\"cpu\",  # Specify your local GPU or CPU.\n",
    "    feature_extractor=tsp,\n",
    ")\n",
    "\n",
    "# Make a forecast on the target column given the input data.\n",
    "finetune_forecast = pipeline(historical_df, future_time_series=controls_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions along with the historical data.\n",
    "\n",
    "The predicted series picks up where the historical data ends, and we can see that it predicts a continuation of the cyclical pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-cast the timestamp to avoid future dtype inference changes\n",
    "historical_df[timestamp_column] = pd.to_datetime(historical_df[timestamp_column])\n",
    "\n",
    "# Plot the historical data and predicted series.\n",
    "plot_predictions(\n",
    "    input_df=historical_df,\n",
    "    predictions_df=finetune_forecast,\n",
    "    freq=tsp.freq,\n",
    "    timestamp_column=timestamp_column,\n",
    "    channel=target_columns[0],\n",
    ")\n",
    "plot_predictions(\n",
    "    input_df=historical_df,\n",
    "    predictions_df=finetune_forecast,\n",
    "    freq=tsp.freq,\n",
    "    timestamp_column=timestamp_column,\n",
    "    channel=target_columns[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "* Library: [Granite TSFM on Github](https://github.com/ibm-granite/granite-tsfm)\n",
    "* Model: [TinyTimeMixer on HuggingFace](https://huggingface.co/ibm-granite/granite-timeseries-ttm-v1)\n",
    "* Dataset: [Bike Sharing Dataset on Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/bike-sharing-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
