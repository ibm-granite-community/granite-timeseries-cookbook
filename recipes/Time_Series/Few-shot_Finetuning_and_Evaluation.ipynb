{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Energy Demand Forecasting with Granite Timeseries - Few-shot Fine-tuning and Evaluation\n",
    "\n",
    "TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. With less than 1 Million parameters, TTM introduces the notion of the first-ever \"tiny\" pre-trained models for Time-Series Forecasting. TTM outperforms several popular benchmarks demanding billions of parameters in zero-shot and few-shot forecasting and can easily be fine-tuned for multi-variate forecasts.\n",
    "\n",
    "In this recipe, we move beyond zero-shot prediction to few-shot fine-tuning and prediction. We use a real-world dataset containing energy demand data from Spain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the TSFM Library \n",
    "\n",
    "The [granite-tsfm library](https://github.com/ibm-granite/granite-tsfm) provides utilities for working with Time Series Foundation Models (TSFM). Here we retrieve and install a tested version of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install the tsfm library\n",
    "! pip install \"tsfm_public[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.17\" -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "From `tsfm_public`, we use the TinyTimeMixer model, forecasting pipeline, and plotting function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "\n",
    "from tsfm_public import (\n",
    "    TimeSeriesForecastingPipeline,\n",
    "    TimeSeriesPreprocessor,\n",
    "    TinyTimeMixerForPrediction,\n",
    "    TrackingCallback,\n",
    "    count_parameters,\n",
    "    get_datasets,\n",
    ")\n",
    "from tsfm_public.toolkit.time_series_preprocessor import prepare_data_splits\n",
    "from tsfm_public.toolkit.visualization import plot_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify configuration variables\n",
    "\n",
    "We provide the names of the timestamp column and the target column to be predicted. The context length (in time steps) is set to match the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_column = \"time\"\n",
    "target_columns = [\"total load actual\", \"generation solar\"]\n",
    "\n",
    "# TTM model branch\n",
    "# Use main for 512-96 model\n",
    "# Use \"1024_96_v1\" for 1024-96 model\n",
    "TTM_MODEL_REVISION = \"main\"\n",
    "context_length = 512  # the max context length for the 512-96 model\n",
    "prediction_length = 96  # the max forecast length for the 512-96 model\n",
    "\n",
    "# Return this percent of the original dataset when getting train/test splits.\n",
    "fewshot_fraction = 0.05\n",
    "\n",
    "# Output directory for writing evaluation results.\n",
    "OUT_DIR = \"/tmp/ttm_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "We'll work with a dataset of hourly electrical demand, generation by type, and weather in Spain. This dataset was originally available [from Kaggle here.](https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather) To simplify access to the data, we will make use of versions available as Hugging Face datasets ([energy consumption](https://huggingface.co/datasets/vitaliy-sharandin/energy-consumption-hourly-spain) and [weather](https://huggingface.co/datasets/vitaliy-sharandin/energy-consumption-weather-hourly-spain)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = \"hf://datasets/vitaliy-sharandin/energy-consumption-hourly-spain/energy_dataset.csv\"\n",
    "DATA_FILE_EXOG_PATH = \"hf://datasets/vitaliy-sharandin/energy-consumption-weather-hourly-spain/weather_features.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "\n",
    "We parse the CSV into a pandas dataframe, filling in any null values, and create a single window containing `context_length` time points. We ensure the timestamp column is a UTC datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from the downloaded file.\n",
    "input_df = pd.read_csv(DATA_FILE_PATH, parse_dates=[timestamp_column])\n",
    "\n",
    "# Fill NA/NaN values by propagating the last valid value.\n",
    "input_df = input_df.ffill()\n",
    "\n",
    "# Show the last few rows of the dataset.\n",
    "input_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also read in the exogenous data. We use one day rolling average weather value as exogenous. Since the exact future weather values are unknown at the time of forecast, it is assumed, that a daily mean forecast is available for each location. We also need to aggregate the weather forecasts which are at a city level, whereas the energy demands are at a country level. We derive the representative weather forecast as a median weather forecast across different cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_timestamp_column = \"dt_iso\"\n",
    "exog_data = pd.read_csv(\n",
    "    DATA_FILE_EXOG_PATH,\n",
    "    parse_dates=[exog_timestamp_column],\n",
    ")\n",
    "\n",
    "keep_cols = [\n",
    "    c for c in exog_data.columns if c not in [\"weather_id\", \"weather_main\", \"weather_description\", \"weather_icon\"]\n",
    "]\n",
    "exog_data = exog_data[keep_cols]\n",
    "value_cols = [c for c in keep_cols if c not in [\"dt_iso\", \"city_name\"]]\n",
    "\n",
    "city_data = []\n",
    "for city in exog_data.city_name.unique():\n",
    "    df = (\n",
    "        exog_data[exog_data.city_name == city].set_index(\"dt_iso\")[value_cols].rolling(window=24, min_periods=1).mean()\n",
    "    )\n",
    "    df[\"city_name\"] = city\n",
    "    df = df.reset_index()\n",
    "    city_data.append(df)\n",
    "\n",
    "exog_df = pd.concat(city_data, axis=0).sort_values(by=\"dt_iso\")\n",
    "exog_df = exog_df.groupby(\"dt_iso\")[value_cols].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we join the target data with the exogenous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = input_df.merge(exog_df, left_on=timestamp_column, right_on=exog_timestamp_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a segment of the target series\n",
    "\n",
    "Here we inspect a preview of the target time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_column in target_columns:\n",
    "    input_df.plot(x=timestamp_column, y=target_column, figsize=(20, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing sets\n",
    "\n",
    "We split the data into training, validation, and test sets. The training set is used to train the model, while the test set is used to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_config = {\"train\": 0.6, \"test\": 0.2}\n",
    "\n",
    "column_specifiers = {\n",
    "    \"timestamp_column\": timestamp_column,\n",
    "    \"id_columns\": [],\n",
    "    \"target_columns\": target_columns,\n",
    "    \"control_columns\": [\n",
    "        \"temp\",\n",
    "        \"temp_min\",\n",
    "        \"temp_max\",\n",
    "        \"pressure\",\n",
    "        \"wind_speed\",\n",
    "        \"wind_deg\",\n",
    "        \"humidity\",\n",
    "        \"rain_1h\",\n",
    "        \"rain_3h\",\n",
    "        \"clouds_all\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "tsp = TimeSeriesPreprocessor(\n",
    "    **column_specifiers,\n",
    "    context_length=context_length,\n",
    "    prediction_length=prediction_length,\n",
    "    scaling=True,\n",
    "    encode_categorical=False,\n",
    "    scaler_type=\"standard\",\n",
    ")\n",
    "\n",
    "train_df, valid_df, test_df = prepare_data_splits(input_df, context_length=context_length, split_config=split_config)\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = get_datasets(\n",
    "    tsp,\n",
    "    input_df,\n",
    "    split_config,\n",
    "    fewshot_fraction=fewshot_fraction,\n",
    "    fewshot_location=\"first\",\n",
    ")\n",
    "print(f\"Data lengths: train = {len(train_dataset)}, val = {len(valid_dataset)}, test = {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model for finetuning\n",
    "The TTM model is hosted on [HuggingFace](https://huggingface.co/ibm-granite/granite-timeseries-ttm-v1), and is loaded into a `TinyTimeMixerForPrediction` model. We have 12 input channels in this example (two targets plus 10 exogenous). Note that we also enable channel mixing in the decoder as well as forecast channel mixing. This allows the decoder to be tuned to capture interactions between the channels. Forecast channel mixing allows incorporating the effect of exogenous values which are known during the forecast horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the model.\n",
    "set_seed(42)\n",
    "finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "    \"ibm-granite/granite-timeseries-ttm-r1\",  # Name of the model on HuggingFace.\n",
    "    num_input_channels=tsp.num_input_channels,\n",
    "    prediction_channel_indices=tsp.prediction_channel_indices,\n",
    "    exogenous_channel_indices=tsp.exogenous_channel_indices,\n",
    "    fcm_use_mixer=True,\n",
    "    fcm_context_length=10,\n",
    "    enable_forecast_channel_mixing=True,\n",
    "    decoder_mode=\"mix_channel\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the TTM Backbone\n",
    "\n",
    "During fine tuning we freeze the backbone and focus on tuning only the parameters in the decoder. This reduces the overall number of parameters being tuned while also maintaining the pre-trained backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Number of params before freezing backbone\",\n",
    "    count_parameters(finetune_forecast_model),\n",
    ")\n",
    "\n",
    "# Freeze the backbone of the model\n",
    "for param in finetune_forecast_model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Count params\n",
    "print(\n",
    "    \"Number of params after freezing the backbone\",\n",
    "    count_parameters(finetune_forecast_model),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Trainer for Finetune Few-shot 5%\n",
    "\n",
    "Configure a Trainer for use in fine-tuning and evaluating the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important parameters\n",
    "learning_rate: float = 0.0001\n",
    "num_epochs: int = 200\n",
    "patience: int = 2\n",
    "batch_size: int = 256\n",
    "\n",
    "print(f\"Using learning rate = {learning_rate}\")\n",
    "finetune_forecast_args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUT_DIR, \"output\"),\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_epochs,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    dataloader_num_workers=8,\n",
    "    report_to=None,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_dir=os.path.join(OUT_DIR, \"logs\"),  # Make sure to specify a logging directory\n",
    "    load_best_model_at_end=True,  # Load the best model when training ends\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "    greater_is_better=False,  # For loss\n",
    "    use_cpu=finetune_forecast_model.device.type != \"cuda\",\n",
    ")\n",
    "\n",
    "# Create the early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=patience,  # Number of epochs with no improvement after which to stop\n",
    "    early_stopping_threshold=0.00001,  # Minimum improvement required to consider as improvement\n",
    ")\n",
    "tracking_callback = TrackingCallback()\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=math.ceil(len(train_dataset) / (batch_size)),\n",
    ")\n",
    "\n",
    "finetune_forecast_trainer = Trainer(\n",
    "    model=finetune_forecast_model,\n",
    "    args=finetune_forecast_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[early_stopping_callback, tracking_callback],\n",
    "    optimizers=(optimizer, scheduler),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "Here we train the model on the fraction of data that was selected above (5% of the training data). This tunes only the weights in the decoder and output layers, as the other weights have been frozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune\n",
    "finetune_forecast_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "Evaluate the fine-tuned model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate forecasts using the finetuned model\n",
    "pipeline = TimeSeriesForecastingPipeline(\n",
    "    finetune_forecast_model,\n",
    "    device=\"cpu\",  # Specify your local GPU or CPU.\n",
    "    feature_extractor=tsp,\n",
    ")\n",
    "\n",
    "# Make a forecast on the target column given the input data.\n",
    "finetune_forecast = pipeline(test_df)\n",
    "finetune_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some standard metrics.\n",
    "def custom_metric(actual, prediction, column_header=\"results\"):\n",
    "    \"\"\"Simple function to compute MSE\"\"\"\n",
    "    a = np.asarray(actual.tolist())\n",
    "    p = np.asarray(prediction.tolist())\n",
    "\n",
    "    mask = ~np.any(np.isnan(a), axis=1)\n",
    "\n",
    "    mse = np.mean(np.square(a[mask, :] - p[mask, :]))\n",
    "    mae = np.mean(np.abs(a[mask, :] - p[mask, :]))\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            column_header: {\n",
    "                \"mean_squared_error\": mse,\n",
    "                \"root_mean_squared_error\": np.sqrt(mse),\n",
    "                \"mean_absolute_error\": mae,\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "custom_metric(\n",
    "    finetune_forecast[\"total load actual\"],\n",
    "    finetune_forecast[\"total load actual_prediction\"],\n",
    "    \"fine-tune forecast (total load)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Predictions vs. Actuals\n",
    "\n",
    "Plot the predictions vs. actuals for some random samples of time intervals in test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plot_predictions(\n",
    "    input_df=test_df,\n",
    "    predictions_df=finetune_forecast,\n",
    "    timestamp_column=timestamp_column,\n",
    "    freq=tsp.freq,\n",
    "    plot_dir=None,\n",
    "    plot_prefix=\"Test\",\n",
    "    channel=\"generation solar\",  # \"total load actual\",\n",
    "    plot_context=2 * prediction_length,\n",
    "    indices=[2464, 3651, 4964, 6224],\n",
    "    num_plots=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(\n",
    "    input_df=test_df,\n",
    "    predictions_df=finetune_forecast,\n",
    "    timestamp_column=timestamp_column,\n",
    "    freq=tsp.freq,\n",
    "    plot_dir=None,\n",
    "    plot_prefix=\"Test\",\n",
    "    channel=\"total load actual\",\n",
    "    plot_context=2 * prediction_length,\n",
    "    indices=[5352, 4964, 1451, 6691],\n",
    "    num_plots=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a forecasting pipeline with the fine-tuned model\n",
    "\n",
    "We extract the appropriate historical data (context) and future values of control variables, and then set up the forecasting pipeline with the model and time series preprocessor.\n",
    "\n",
    "We choose a context window near the end of the test dataset, while still leaving enough known data from which we extract the controls. This is meant to simulate the case where you have historical information for the inputs and also provide future values of the controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df = input_df.iloc[-context_length-prediction_length:-prediction_length].copy()\n",
    "controls_df = input_df.iloc[-prediction_length:][column_specifiers[\"control_columns\"]].copy()\n",
    "\n",
    "# Create a pipeline.\n",
    "pipeline = TimeSeriesForecastingPipeline(\n",
    "    finetune_forecast_trainer.model,\n",
    "    feature_extractor=tsp,\n",
    "    device=\"cpu\",  # Specify your local GPU or CPU.\n",
    ")\n",
    "\n",
    "# Make a forecast on the target column given the input data.\n",
    "future_forecast = pipeline(historical_df, future_time_series=controls_df)\n",
    "future_forecast.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions along with the historical data.\n",
    "\n",
    "The predicted series picks up where the historical data ends, and we can see that it predicts a continuation of the cyclical pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-cast the timestamp to avoid future dtype inference changes\n",
    "historical_df[timestamp_column] = pd.to_datetime(historical_df[timestamp_column])\n",
    "\n",
    "# Plot the historical data and predicted series.\n",
    "plot_predictions(\n",
    "    input_df=historical_df,\n",
    "    predictions_df=future_forecast,\n",
    "    freq=tsp.freq,\n",
    "    timestamp_column=timestamp_column,\n",
    "    channel=target_columns[0],\n",
    ")\n",
    "plot_predictions(\n",
    "    input_df=historical_df,\n",
    "    predictions_df=future_forecast,\n",
    "    freq=tsp.freq,\n",
    "    timestamp_column=timestamp_column,\n",
    "    channel=target_columns[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "* Library: [Granite TSFM on Github](https://github.com/ibm-granite/granite-tsfm)\n",
    "* Model: [TinyTimeMixer on HuggingFace](https://huggingface.co/ibm-granite/granite-timeseries-ttm-v1)\n",
    "* Dataset: [Hourly Energy Demand Generation and Weather on Kaggle](https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
