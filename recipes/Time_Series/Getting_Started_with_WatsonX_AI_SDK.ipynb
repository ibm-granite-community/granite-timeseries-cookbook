{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Time Series Models on IBM WatsonX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates using the WatsonX SDK to perform inference calls against a model hosted remotely on [WatsonX](https://www.ibm.com/products/watsonx-ai)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "> **NOTE**: When running this recipe in [Colab](https://colab.research.google.com/), you may see an error about dependency conflicts with `google-colab 1.0.0`. You can safely ignore this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils\n",
    "!pip install ibm-watsonx-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "from ibm_watsonx_ai.foundation_models import TSModelInference\n",
    "from ibm_watsonx_ai.foundation_models.schema import TSForecastParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide the environment variables\n",
    "\n",
    "There are three ways to provide the environment variables required. In order of precedence:\n",
    "\n",
    "1. Directly as an environment variable in the python environment where the jupyter notebook is running.\n",
    "2. As a Google Colab secret, if you are running the notebook in Colab.\n",
    "3. Supplied by the user in a prompt during execution of the notebook.\n",
    "\n",
    "#### Provide your API Key\n",
    "\n",
    "Obtain your `WATSONX_APIKEY` by generating a [Platform API Key](https://www.ibm.com/docs/en/watsonx/watsonxdata/1.0.x?topic=started-generating-api-keys) on the watsonx.data web client.\n",
    "\n",
    "#### Provide your Project Id\n",
    "\n",
    "Get your `WATSONX_PROJECT_ID` from the [WatsonX](https://www.ibm.com/watsonx) web client by following [these instructions](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-project-id.html?context=wx).\n",
    "\n",
    "#### Provide your base WatsonX URL\n",
    "\n",
    "Get your `WATSONX_URL` by viewing the details for the service instance from the Cloud Pak for Data web client, as described in [these watsonx.ai setup instructions](https://ibm.github.io/watsonx-ai-python-sdk/setup_cpd.html).\n",
    "\n",
    "As an example, your `WATSONX_URL` may be `https://us-south.ml.cloud.ibm.com` for the Dallas zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = Credentials(\n",
    "    api_key=get_env_var(\"WATSONX_APIKEY\"),\n",
    "    url=get_env_var(\"WATSONX_URL\"),\n",
    ")\n",
    "client = APIClient(credentials)\n",
    "client.set.default_project(get_env_var(\"WATSONX_PROJECT_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in client.foundation_models.get_time_series_model_specs()[\"resources\"]:\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f'model_id: {model[\"model_id\"]}')\n",
    "    print(f'functions: {model[\"functions\"]}')\n",
    "    print(f'long_description: {model[\"long_description\"]}')\n",
    "    print(f'label: {model[\"label\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model_id = client.foundation_models.TimeSeriesModels.GRANITE_TTM_512_96_R2\n",
    "\n",
    "ts_model = TSModelInference(model_id=ts_model_id, api_client=client)\n",
    "context_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "We'll work with a [bike sharing dataset](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset) available from the UCI Machine learning repository. This dataset includes the count of rental bikes between the years 2011 and 2012 in the Capital bike share system with the corresponding weather and seasonal information.\n",
    "\n",
    "You can download the source code to a temporary directory by running the following commands. Later you can clean up any downloaded files by removing the `temp` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# curl https://archive.ics.uci.edu/static/public/275/$BIKE_SHARING -o $BIKE_SHARING && \\\n",
    "BIKE_SHARING=bike+sharing+dataset.zip\n",
    "test -d temp || ( \\\n",
    "  mkdir -p temp && \\\n",
    "  cd temp && \\\n",
    "    wget https://archive.ics.uci.edu/static/public/275/$BIKE_SHARING -O $BIKE_SHARING && \\\n",
    "    unzip -o $BIKE_SHARING && \\\n",
    "  rm -f $BIKE_SHARING && \\\n",
    "  cd - \\\n",
    ") && ls -l temp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = \"temp/hour.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "\n",
    "We parse the CSV into a pandas dataframe, filling in any null values, and create a single window containing `context_length` time points. We ensure the timestamp column is a UTC datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_column = \"dteday\"\n",
    "target_columns = [\"casual\", \"registered\"]\n",
    "\n",
    "# Read in the data from the downloaded file.\n",
    "input_df = pd.read_csv(DATA_FILE_PATH, parse_dates=[timestamp_column])\n",
    "\n",
    "# Fix missing hours in original dataset date column\n",
    "input_df[timestamp_column] = input_df[timestamp_column] + input_df.hr.apply(lambda x: pd.Timedelta(x, unit=\"hr\"))\n",
    "\n",
    "# Show the last few rows of the dataset.\n",
    "input_df[timestamp_column] = input_df[timestamp_column].apply(lambda x: x.isoformat())\n",
    "input_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasting_params = TSForecastParameters(\n",
    "    timestamp_column=timestamp_column, freq=\"1h\", target_columns=target_columns, prediction_length=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ts_model.forecast(data=input_df[:context_length], params=forecasting_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results[\"results\"][0], columns=[timestamp_column] + target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
