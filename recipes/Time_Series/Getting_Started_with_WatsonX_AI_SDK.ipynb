{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Time Series Models on IBM WatsonX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates using the WatsonX SDK to perform inference calls against a model hosted remotely on [WatsonX](https://www.ibm.com/products/watsonx-ai)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "> **NOTE**: When running this recipe in [Colab](https://colab.research.google.com/), you may see an error about dependency conflicts with `google-colab 1.0.0`. You can safely ignore this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils\n",
    "!pip install ibm-watsonx-ai\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "from ibm_watsonx_ai.foundation_models import TSModelInference\n",
    "from ibm_watsonx_ai.foundation_models.schema import TSForecastParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide the environment variables\n",
    "\n",
    "There are three ways to provide the environment variables required. In order of precedence:\n",
    "\n",
    "1. Directly as an environment variable in the python environment where the jupyter notebook is running.\n",
    "2. As a Google Colab secret, if you are running the notebook in Colab.\n",
    "3. Supplied by the user in a prompt during execution of the notebook.\n",
    "\n",
    "#### Provide your API Key\n",
    "\n",
    "Obtain your `WATSONX_APIKEY` by generating a [Platform API Key](https://www.ibm.com/docs/en/watsonx/watsonxdata/1.0.x?topic=started-generating-api-keys) on the watsonx.data web client.\n",
    "\n",
    "#### Provide your Project Id\n",
    "\n",
    "Get your `WATSONX_PROJECT_ID` from the [WatsonX](https://www.ibm.com/watsonx) web client by following [these instructions](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-project-id.html?context=wx).\n",
    "\n",
    "#### Provide your base WatsonX URL\n",
    "\n",
    "Get your `WATSONX_URL` by viewing the details for the service instance from the Cloud Pak for Data web client, as described in [these watsonx.ai setup instructions](https://ibm.github.io/watsonx-ai-python-sdk/setup_cpd.html).\n",
    "\n",
    "As an example, your `WATSONX_URL` may be `https://us-south.ml.cloud.ibm.com` for the Dallas zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = Credentials(\n",
    "    api_key=get_env_var(\"WATSONX_APIKEY\"),\n",
    "    url=get_env_var(\"WATSONX_URL\"),\n",
    ")\n",
    "client = APIClient(credentials)\n",
    "client.set.default_project(get_env_var(\"WATSONX_PROJECT_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in client.foundation_models.get_time_series_model_specs()[\"resources\"]:\n",
    "    pprint.pp(\"--------------------------------------------------\")\n",
    "    pprint.pp(f'model_id: {model[\"model_id\"]}')\n",
    "    pprint.pp(f'functions: {model[\"functions\"]}')\n",
    "    pprint.pp(f'long_description: {model[\"long_description\"]}')\n",
    "    pprint.pp(f'label: {model[\"label\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model_id = client.foundation_models.TimeSeriesModels.GRANITE_TTM_512_96_R2\n",
    "\n",
    "ts_model = TSModelInference(model_id=ts_model_id, api_client=client)\n",
    "context_length = 512\n",
    "prediction_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "We'll work with a [bike sharing dataset](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset) available from the UCI Machine learning repository. This dataset includes the count of rental bikes between the years 2011 and 2012 in the Capital bike share system with the corresponding weather and seasonal information.\n",
    "\n",
    "You can download the source code to a temporary directory by running the following commands. Later you can clean up any downloaded files by removing the `temp` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# curl https://archive.ics.uci.edu/static/public/275/$BIKE_SHARING -o $BIKE_SHARING && \\\n",
    "BIKE_SHARING=bike+sharing+dataset.zip\n",
    "test -d temp || ( \\\n",
    "  mkdir -p temp && \\\n",
    "  cd temp && \\\n",
    "    wget https://archive.ics.uci.edu/static/public/275/$BIKE_SHARING -O $BIKE_SHARING && \\\n",
    "    unzip -o $BIKE_SHARING && \\\n",
    "  rm -f $BIKE_SHARING && \\\n",
    "  cd - \\\n",
    ") && ls -l temp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = \"temp/hour.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "\n",
    "We parse the CSV into a pandas dataframe, filling in any null values, and create a single window containing `context_length` time points. We ensure the timestamp column is a UTC datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_column = \"dteday\"\n",
    "target_columns = [\"casual\", \"registered\"]\n",
    "\n",
    "# Read in the data from the downloaded file.\n",
    "input_df = pd.read_csv(DATA_FILE_PATH, parse_dates=[timestamp_column])\n",
    "\n",
    "# Fix missing hours in original dataset date column\n",
    "input_df[timestamp_column] = input_df[timestamp_column] + input_df.hr.apply(lambda x: pd.Timedelta(x, unit=\"hr\"))\n",
    "\n",
    "# Show the last few rows of the dataset.\n",
    "input_df[timestamp_column] = input_df[timestamp_column].apply(lambda x: x.isoformat())\n",
    "input_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset for the request\n",
    "\n",
    "Rather than making a single request by passing one context length of the data to the watsonx.ai service, we select a few starting indices and provide a single payload to make multiple forecasts at once. We add an id column to distinguish these context windows.\n",
    "\n",
    "In the cell below, we construct the `input_data` DataFrame containing the context windows, while `ground_truth_data` contains the true values of the target columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = [2173, 10635, 10935, 14239]  # randomly chosen starting points\n",
    "id_column = \"id\"\n",
    "\n",
    "input_data = []\n",
    "ground_truth_data = []\n",
    "for i in start_index:\n",
    "    df = input_df.iloc[i : i + context_length, :].copy()\n",
    "    df[id_column] = f\"id_{i}\"\n",
    "    input_data.append(df)\n",
    "    df = input_df.iloc[i + context_length : i + context_length + prediction_length, :].copy()\n",
    "    df[id_column] = f\"id_{i}\"\n",
    "    ground_truth_data.append(df)\n",
    "\n",
    "input_data = pd.concat(input_data)\n",
    "ground_truth_data = pd.concat(ground_truth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the arguments for the forecast request\n",
    "\n",
    "We first construct the forecast parameters structure and then call the forecast method. The forecast parameters specify the id column, timestamp column, target columns, frequency, and prediction length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasting_params = TSForecastParameters(\n",
    "    id_columns=[id_column],\n",
    "    timestamp_column=timestamp_column,\n",
    "    freq=\"1h\",\n",
    "    target_columns=target_columns,\n",
    "    prediction_length=prediction_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the forecast request and plot results\n",
    "\n",
    "Here we use the forecast method to get the forecast for our data, then we plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ts_model.forecast(data=input_data, params=forecasting_params)\n",
    "df_out = pd.DataFrame(results[\"results\"][0], columns=[id_column, timestamp_column] + target_columns)\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a more beautiful style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "# how much history to plot\n",
    "history = 60\n",
    "\n",
    "grps = input_data.groupby(id_column)\n",
    "\n",
    "num_plots = len(grps)\n",
    "fig, axs = plt.subplots(num_plots, 1, figsize=(10, 2 * num_plots))\n",
    "\n",
    "\n",
    "for i, (grp_name, grp) in enumerate(grps):\n",
    "    # concatenate ground truth historical data with the ground truth for the future targets\n",
    "    gt = ground_truth_data[ground_truth_data[id_column] == grp_name]\n",
    "    gt_dt = pd.concat([grp[timestamp_column].iloc[-history:], gt[timestamp_column]])\n",
    "    gt_val = pd.concat([grp[target_columns[0]].iloc[-history:], gt[target_columns[0]]])\n",
    "\n",
    "    # plot ground truth\n",
    "    axs[i].plot(pd.to_datetime(gt_dt), gt_val, label=\"Ground truth\", linestyle=\"-\", color=\"blue\", linewidth=2)\n",
    "\n",
    "    # plot forecasts\n",
    "    pred = df_out[df_out[id_column] == grp_name]\n",
    "    axs[i].plot(\n",
    "        pd.to_datetime(pred[timestamp_column]),\n",
    "        pred[target_columns[0]],\n",
    "        label=\"Prediction\",\n",
    "        linestyle=\"--\",\n",
    "        color=\"orange\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(grp_name)\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
