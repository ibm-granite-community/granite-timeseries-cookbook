{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# BasicMotions Classification with Granite Time Series TSPulse\n",
    "\n",
    "TSPulse are compact pre-trained models specialized for various multivariate time-series analysis tasks namely classification, anomaly-detection, imputation and similarity search, open-sourced by IBM Research. With 1 Million parameters, TSPulse introduces the notion of the first-ever \"tiny\" pre-trained models for Time-Series tasks like classification, anomaly-detection, imputation and similarity search. TSPulse outperforms several popular benchmarks demanding billions of parameters in zero-shot and few-shot setting and can easily be fine-tuned for different downstream tasks.\n",
    "\n",
    "In this recipe, we demonstrates the usage of a pre-trained TSPulse model for time-series classification task. This example makes use of the [BasicMotions](https://www.timeseriesclassification.com/description.php?Dataset=BasicMotions) dataset from the [UEA](https://arxiv.org/abs/1811.00075) multivariate time-series classification archive.\n",
    "\n",
    "The data was generated as part of a student project where four students performed four activities whilst wearing a smart watch. The watch collects 3D accelerometer and a 3D gyroscope. The data order is accelerometer x, y, z then gyroscope x, y, z. There are four different classes: walking, resting, running and badminton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install the TSFM Library\n",
    "\n",
    "The [granite-tsfm library](https://github.com/ibm-granite/granite-tsfm) provides utilities for working with Time Series Foundation Models (TSFM). Here the pinned version is retrieved and installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the tsfm library\n",
    "! pip install \"granite-tsfm[notebooks]==0.3.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "from transformers.trainer_utils import RemoveColumnsCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Imports from `tsfm_public`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfm_public.models.tspulse import TSPulseForClassification\n",
    "from tsfm_public.toolkit.dataset import ClassificationDFDataset\n",
    "from tsfm_public.toolkit.lr_finder import optimal_lr_finder\n",
    "from tsfm_public.toolkit.time_series_classification_preprocessor import TimeSeriesClassificationPreprocessor\n",
    "from tsfm_public.toolkit.util import convert_tsfile_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We will download the BasicMotions dataset and extract the train and test `.ts files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.timeseriesclassification.com/aeon-toolkit/BasicMotions.zip\"\n",
    "\n",
    "extract_dir = \"BasicMotions\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "print(\"Downloading ZIP file...\")\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "    ts_files = [f for f in zip_ref.namelist() if f.endswith(\".ts\")]\n",
    "\n",
    "    print(f\"Extracting {len(ts_files)} .ts files...\")\n",
    "    for file in ts_files:\n",
    "        zip_ref.extract(file, extract_dir)\n",
    "\n",
    "print(f\"Extraction completed. Files are saved in '{extract_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Read in the data and train the preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"BasicMotions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{dataset_name}/{dataset_name}_TRAIN.ts\"\n",
    "\n",
    "df_base = convert_tsfile_to_dataframe(\n",
    "    path,\n",
    "    return_separate_X_and_y=False,\n",
    ")\n",
    "label_column = \"class_vals\"\n",
    "input_columns = [f\"dim_{i}\" for i in range(df_base.shape[1] - 1)]\n",
    "\n",
    "tsp = TimeSeriesClassificationPreprocessor(\n",
    "    input_columns=input_columns,\n",
    "    label_column=label_column,\n",
    "    scaling=True,\n",
    ")\n",
    "\n",
    "tsp.train(df_base)\n",
    "df_prep = tsp.preprocess(df_base)\n",
    "base_dataset = ClassificationDFDataset(\n",
    "    df_prep,\n",
    "    id_columns=[],\n",
    "    timestamp_column=None,\n",
    "    input_columns=input_columns,\n",
    "    label_column=label_column,\n",
    "    context_length=512,\n",
    "    static_categorical_columns=[],\n",
    "    stride=1,\n",
    "    enable_padding=False,\n",
    "    full_series=True,\n",
    ")\n",
    "\n",
    "path = f\"{dataset_name}/{dataset_name}_TRAIN.ts\"\n",
    "\n",
    "df_test = convert_tsfile_to_dataframe(\n",
    "    path,\n",
    "    return_separate_X_and_y=False,\n",
    ")\n",
    "label_column = \"class_vals\"\n",
    "input_columns = [f\"dim_{i}\" for i in range(df_test.shape[1] - 1)]\n",
    "\n",
    "tsp = TimeSeriesClassificationPreprocessor(\n",
    "    input_columns=input_columns,\n",
    "    label_column=label_column,\n",
    "    scaling=True,\n",
    ")\n",
    "\n",
    "tsp.train(df_test)\n",
    "df_prep = tsp.preprocess(df_test)\n",
    "\n",
    "test_dataset = ClassificationDFDataset(\n",
    "    df_prep,\n",
    "    id_columns=[],\n",
    "    timestamp_column=None,\n",
    "    input_columns=input_columns,\n",
    "    label_column=label_column,\n",
    "    context_length=512,\n",
    "    static_categorical_columns=[],\n",
    "    stride=1,\n",
    "    enable_padding=False,\n",
    "    full_series=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Creating a validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(base_dataset)\n",
    "print(dataset_size)\n",
    "split_valid_ratio = 0.1\n",
    "val_size = int(split_valid_ratio * dataset_size)  # 10% valid split\n",
    "train_size = dataset_size - val_size\n",
    "train_dataset, valid_dataset = random_split(base_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Configs for the TSPulse model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Hyperparameters to Optimize and suggested values :\n",
    "head_reduce_d_model = 1, 2\n",
    "\n",
    "decoder_mode = mix_channel, common_channel\n",
    "\n",
    "head_gated_attention_activation = softmax, sigmoid\n",
    "\n",
    "mask_ratio = 0, 0.3\n",
    "\n",
    "channel_virtual_expand_scale = 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    \"head_gated_attention_activation\": \"softmax\",\n",
    "    \"channel_virtual_expand_scale\": 2,\n",
    "    \"mask_ratio\": 0.3,\n",
    "    \"head_reduce_d_model\": 1,\n",
    "    \"disable_mask_in_classification_eval\": True,\n",
    "    \"fft_time_consistent_masking\": True,\n",
    "    \"decoder_mode\": \"mix_channel\",\n",
    "    \"head_aggregation_dim\": \"patch\",\n",
    "    \"head_aggregation\": None,\n",
    "    \"loss\": \"cross_entropy\",\n",
    "    \"ignore_mismatched_sizes\": True,\n",
    "}\n",
    "\n",
    "config_dict[\"num_input_channels\"] = tsp.num_input_channels\n",
    "config_dict[\"num_targets\"] = df_base[\"class_vals\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Getting the Pretrained TSPulse Model from HuggingFace with above configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSPulseForClassification.from_pretrained(\n",
    "    \"ibm-granite/granite-timeseries-tspulse-r1\", revision=\"tspulse-block-dualhead-512-p16-r1\", **config_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model = model.to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Backbone of the pre-trained model is freezed and the classifier head along with the input patch embedding layer is finetuned on the classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing Backbone except patch embedding layer....\n",
    "\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.backbone.time_encoding.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.backbone.fft_encoding.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Finetuning the classifier head and patch embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"tspulse_finetuned_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-98912:t-8353009472:lr_finder.py:optimal_lr_finder:LR Finder: Running learning rate (LR) finder algorithm. If the suggested LR is very low, we suggest setting the LR manually.\n",
      "INFO:p-98912:t-8353009472:lr_finder.py:optimal_lr_finder:LR Finder: Using mps.\n",
      "INFO:p-98912:t-8353009472:lr_finder.py:optimal_lr_finder:LR Finder: Suggested learning rate = 0.004037017258596558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested LR :  0.004037017258596558\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='356' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [356/400 46:59 < 05:50, 0.13 it/s, Epoch 178/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.639900</td>\n",
       "      <td>1.453306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.438300</td>\n",
       "      <td>1.445638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.420800</td>\n",
       "      <td>1.437968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.459900</td>\n",
       "      <td>1.432224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.396900</td>\n",
       "      <td>1.428331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.413900</td>\n",
       "      <td>1.426167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.420100</td>\n",
       "      <td>1.426166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.550800</td>\n",
       "      <td>1.427413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.432000</td>\n",
       "      <td>1.428102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.432200</td>\n",
       "      <td>1.427513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.478000</td>\n",
       "      <td>1.426048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.292300</td>\n",
       "      <td>1.423473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.322300</td>\n",
       "      <td>1.424342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.213100</td>\n",
       "      <td>1.427539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.159100</td>\n",
       "      <td>1.436816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.060100</td>\n",
       "      <td>1.446607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.091200</td>\n",
       "      <td>1.441536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.164200</td>\n",
       "      <td>1.419283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.084800</td>\n",
       "      <td>1.384548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.059300</td>\n",
       "      <td>1.320123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.979700</td>\n",
       "      <td>1.232783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.921300</td>\n",
       "      <td>1.146906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.987200</td>\n",
       "      <td>1.073195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>1.011746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.823300</td>\n",
       "      <td>0.955031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.711900</td>\n",
       "      <td>0.898865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.707800</td>\n",
       "      <td>0.869616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.504200</td>\n",
       "      <td>0.836212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.439800</td>\n",
       "      <td>0.780413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.639821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.374200</td>\n",
       "      <td>0.527213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.480379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>0.456565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.385372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.302945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.331878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.079900</td>\n",
       "      <td>0.346033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.339857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.315303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.152956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.099610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.115569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.158272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.094213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.055601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.051443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.059635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.056245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.044631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.072916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.132457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.123472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.102167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.089179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.051932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.058287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.096944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.101712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.077231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.048469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.023316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.013692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.009152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.007506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.009395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.013297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.017547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.018828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.018991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.018569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.016002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.013317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.011652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.010674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.007814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.012548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.005862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.002127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.002558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.004588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.004968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.006956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.005280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.007087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.016613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.021444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.036337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.025013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.025288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.021512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.052308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.104700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.091845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.065498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.043243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.028275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.020108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.019708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.020374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.021043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.024295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.031856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.038888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.042329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.044966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.040708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.033279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.030622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.025987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.020501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.017051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.014982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.013498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.012563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.012730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.013491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.014173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.014783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.015869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.016323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.016727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.016988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.016735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.016115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.015434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.014995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.014653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.014476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.014345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.014203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.013231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.011789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.010679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.009528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.008712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.008125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.007716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.007631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.007751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.007976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.008179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.008270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.008377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.008517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.008637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.008728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.008801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.008835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.008812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.008764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.008704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.008690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.008684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.008697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.008701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.008703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.008803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.008899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.008974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.009033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.009049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.009080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.009105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.009114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.009132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.009150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.009170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.009209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.009241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.009280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.009308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.009326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.009335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=356, training_loss=0.20193236314891405, metrics={'train_runtime': 2822.1258, 'train_samples_per_second': 2.551, 'train_steps_per_second': 0.142, 'total_flos': 43968114081792.0, 'train_loss': 0.20193236314891405, 'epoch': 178.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "suggested_lr = None\n",
    "\n",
    "train_dict = {\"per_device_train_batch_size\": 32, \"num_train_epochs\": 200, \"eval_accumulation_steps\": None}\n",
    "EPOCHS = train_dict[\"num_train_epochs\"]\n",
    "BATCH_SIZE = train_dict[\"per_device_train_batch_size\"]\n",
    "eval_accumulation_steps = train_dict[\"eval_accumulation_steps\"]\n",
    "NUM_WORKERS = 1\n",
    "NUM_GPUS = 1\n",
    "\n",
    "set_seed(42)\n",
    "if suggested_lr is None:\n",
    "    lr, model = optimal_lr_finder(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    suggested_lr = lr\n",
    "print(\"Suggested LR : \", suggested_lr)\n",
    "finetune_args = TrainingArguments(\n",
    "    output_dir=temp_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=suggested_lr,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_accumulation_steps=eval_accumulation_steps,\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_dir=os.path.join(OUT_DIR, \"output\"),  # Make sure to specify a logging directory\n",
    "    load_best_model_at_end=True,  # Load the best model when training ends\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "    greater_is_better=False,  # For loss\n",
    ")\n",
    "\n",
    "# Create the early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=100,  # Number of epochs with no improvement after which to stop\n",
    "    early_stopping_threshold=0.0001,  # Minimum improvement required to consider as improvement\n",
    ")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=suggested_lr)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    suggested_lr,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=math.ceil(len(train_dataset) / (BATCH_SIZE * NUM_GPUS)),\n",
    ")\n",
    "\n",
    "finetune_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=finetune_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    optimizers=(optimizer, scheduler),\n",
    ")\n",
    "\n",
    "# Fine tune\n",
    "finetune_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "predictions_dict = finetune_trainer.predict(test_dataset)\n",
    "preds_np = predictions_dict.predictions[0]\n",
    "\n",
    "remove_columns_collator = RemoveColumnsCollator(\n",
    "    data_collator=default_data_collator,\n",
    "    signature_columns=[\"target_values\"],\n",
    "    logger=None,\n",
    "    description=None,\n",
    "    model_name=\"temp\",\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=remove_columns_collator)\n",
    "target_list = []\n",
    "for batch in test_dataloader:\n",
    "    batch_labels = batch[\"target_values\"].numpy()\n",
    "    target_list.append(batch_labels)\n",
    "targets_np = np.concatenate(target_list, axis=0)\n",
    "test_accuracy = np.mean(targets_np == np.argmax(preds_np, axis=1))\n",
    "print(\"test_accuracy : \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Using Classification Pipeline for inference with the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfm_public.toolkit.time_series_classification_pipeline import TimeSeriesClassificationPipeline\n",
    "\n",
    "pipe = TimeSeriesClassificationPipeline(finetune_trainer.model, feature_extractor=tsp, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "keep_output": true
   },
   "outputs": [],
   "source": [
    "pipe(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Actual ground truth labels vs the labels predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_vals</th>\n",
       "      <th>class_vals_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standing</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standing</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standing</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standing</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>standing</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>standing</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>standing</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>standing</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>standing</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>standing</td>\n",
       "      <td>standing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>running</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>running</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>running</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>running</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>running</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>running</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>running</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>running</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>running</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>running</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>walking</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>walking</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>walking</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>walking</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>walking</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>walking</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>walking</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>walking</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>walking</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>walking</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>badminton</td>\n",
       "      <td>badminton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>badminton</td>\n",
       "      <td>badminton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>badminton</td>\n",
       "      <td>badminton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>badminton</td>\n",
       "      <td>badminton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>badminton</td>\n",
       "      <td>badminton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>badminton</td>\n",
       "      <td>badminton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>badminton</td>\n",
       "      <td>badminton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>badminton</td>\n",
       "      <td>badminton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>badminton</td>\n",
       "      <td>badminton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>badminton</td>\n",
       "      <td>badminton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_vals class_vals_prediction\n",
       "0    standing              standing\n",
       "1    standing              standing\n",
       "2    standing              standing\n",
       "3    standing              standing\n",
       "4    standing              standing\n",
       "5    standing              standing\n",
       "6    standing              standing\n",
       "7    standing              standing\n",
       "8    standing              standing\n",
       "9    standing              standing\n",
       "10    running               running\n",
       "11    running               running\n",
       "12    running               running\n",
       "13    running               running\n",
       "14    running               running\n",
       "15    running               running\n",
       "16    running               running\n",
       "17    running               running\n",
       "18    running               running\n",
       "19    running               running\n",
       "20    walking               walking\n",
       "21    walking               walking\n",
       "22    walking               walking\n",
       "23    walking               walking\n",
       "24    walking               walking\n",
       "25    walking               walking\n",
       "26    walking               walking\n",
       "27    walking               walking\n",
       "28    walking               walking\n",
       "29    walking               walking\n",
       "30  badminton             badminton\n",
       "31  badminton             badminton\n",
       "32  badminton             badminton\n",
       "33  badminton             badminton\n",
       "34  badminton             badminton\n",
       "35  badminton             badminton\n",
       "36  badminton             badminton\n",
       "37  badminton             badminton\n",
       "38  badminton             badminton\n",
       "39  badminton             badminton"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = pipe(df_test)\n",
    "out[[\"class_vals\", \"class_vals_prediction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
